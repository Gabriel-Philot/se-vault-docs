# Spark Broadcast Variables: Best Practices & Performance Guide

## O Que SÃ£o Broadcast Variables

Broadcast variables sÃ£o um mecanismo do Spark para enviar dados do **driver â†’ workers** de forma eficiente, armazenando uma cÃ³pia read-only em cada executor.

```
DRIVER                           WORKERS (executors)
------                           -------------------
data = {...}                     [Executor 1: cÃ³pia local]
bc = sc.broadcast(data)   â†’      [Executor 2: cÃ³pia local]
                                 [Executor 3: cÃ³pia local]
```

**Sem broadcast**: Cada task serializa os dados (centenas de cÃ³pias)  
**Com broadcast**: Uma cÃ³pia por executor (otimizado via BitTorrent-like protocol)

---

## Regra de Ouro: DataFrame > Collect

### âŒ Anti-Pattern: Collect + Broadcast Manual

```python
# O que analistas costumam fazer (EVITAR)
small_data = spark.sql("SELECT id, value FROM dim_table").collect()
bc_data = spark.sparkContext.broadcast({
    row['id']: row['value'] for row in small_data
})

@udf(returnType=StringType())
def enrich(id):
    return bc_data.value.get(id, 'unknown')

result = df.withColumn('enriched_value', enrich('customer_id'))
```

**Problemas**:
1. Driver coleta todos os dados (pode dar OOM se tabela crescer)
2. ConversÃ£o Row â†’ dict Ã© lenta e desperdiÃ§a memÃ³ria
3. UDF Python Ã© ~10x mais lenta que funÃ§Ãµes nativas
4. Spark nÃ£o consegue otimizar (operador opaco)

---

### âœ… Best Practice: DataFrame + Broadcast Hint

```python
# RefatoraÃ§Ã£o correta
dim_df = spark.sql("SELECT id, value FROM dim_table")
result = df.join(broadcast(dim_df), df.customer_id == dim_df.id, 'left')
```

**Vantagens**:
1. âœ… **Catalyst otimiza** - Escolhe algoritmo de join ideal (hash join)
2. âœ… **Predicate pushdown** - Filtros sÃ£o empurrados pra leitura
3. âœ… **Codegen** - Join compilado Ã© ~10x mais rÃ¡pido que UDF
4. âœ… **Lazy evaluation** - SÃ³ lÃª o necessÃ¡rio
5. âœ… **Seguro** - Se dados forem grandes, Spark avisa/ajusta

---

## Quando Broadcast Hint Funciona

```python
from pyspark.sql.functions import broadcast

# âœ… Caso 1: Lookup simples (id â†’ valor)
customers_df = spark.table('dim_customers').select('id', 'tier', 'discount')
enriched = sales_df.join(broadcast(customers_df), 'customer_id')

# âœ… Caso 2: Filtros complexos (semi-join)
vip_ids = spark.sql("SELECT id FROM customers WHERE tier = 'VIP'")
vip_sales = sales_df.join(broadcast(vip_ids), 'customer_id', 'left_semi')

# âœ… Caso 3: MÃºltiplas dimensÃµes pequenas
result = fact_df \
    .join(broadcast(dim_date), 'date_id') \
    .join(broadcast(dim_product), 'product_id') \
    .join(broadcast(dim_store), 'store_id')
```

**Limite recomendado**: Dados < 1GB apÃ³s compressÃ£o (ajustÃ¡vel via `spark.sql.autoBroadcastJoinThreshold`)

---

## Ãšnica ExceÃ§Ã£o: LÃ³gica Python Complexa

### Quando Collect + Broadcast Manual Faz Sentido

```python
# âœ… Caso vÃ¡lido: Regex precompilado + lÃ³gica externa

import re
from custom_lib import validate_with_external_api

# Regras de validaÃ§Ã£o que precisam ser compiladas uma vez
rules = spark.sql("SELECT pattern, action, priority FROM validation_rules").collect()

bc_rules = spark.sparkContext.broadcast([
    {
        'regex': re.compile(r['pattern']),  # Precompila regex (Python puro)
        'action': r['action'],
        'priority': r['priority']
    }
    for r in rules
])

@udf(returnType=StringType())
def apply_validation(text):
    """LÃ³gica que NÃƒO tem equivalente SQL."""
    for rule in sorted(bc_rules.value, key=lambda x: x['priority']):
        match = rule['regex'].search(text)
        if match:
            # Chama API externa (impossÃ­vel em SQL puro)
            return validate_with_external_api(rule['action'], match.group(0))
    return 'no_match'

df_validated = df.withColumn('validation_result', apply_validation('text_field'))
```

**Justificativas vÃ¡lidas para collect + broadcast**:
- âœ… Precisa precomputar objetos Python (regex compilado, modelos ML)
- âœ… Usa bibliotecas externas indisponÃ­veis no Spark SQL
- âœ… LÃ³gica procedural complexa (loops, condicionais aninhados)
- âœ… Dados caem confortavelmente em memÃ³ria (< 100MB)

---

## Performance Comparison

### Benchmark: Enriquecer 1 bilhÃ£o de registros com lookup de 10k IDs

```python
# Setup
big_df = spark.range(1_000_000_000).withColumn('customer_id', (col('id') % 10000))
dim = spark.range(10_000).withColumn('tier', lit('Gold'))

# Teste 1: Collect + Broadcast + UDF â±ï¸ ~8 min
data = dim.collect()
bc = spark.sparkContext.broadcast({r['id']: r['tier'] for r in data})
@udf(...)
def lookup(id): return bc.value.get(id)
result = big_df.withColumn('tier', lookup('customer_id'))

# Teste 2: DataFrame + Broadcast Hint âš¡ ~2 min (4x mais rÃ¡pido)
result = big_df.join(broadcast(dim.select('id', 'tier')), 
                      big_df.customer_id == dim.id, 'left')

# Teste 3: Join sem Broadcast ğŸŒ ~15 min (shuffle gigante)
result = big_df.join(dim, big_df.customer_id == dim.id, 'left')
```

---

## Anti-Patterns Comuns

### âŒ 1. Broadcast de Dados Grandes

```python
# ERRADO: Tabela de 5GB
huge_df = spark.table('fact_sales_history')  # 5GB
result = df.join(broadcast(huge_df), 'id')  # OOM nos executors!

# CERTO: Deixa Spark decidir ou usa sort-merge join
result = df.join(huge_df, 'id')  # Spark usa shuffle se necessÃ¡rio
```

---

### âŒ 2. Collect Dentro de Loop

```python
# ERRADO: MÃºltiplos collects
for region in ['BR', 'US', 'EU']:
    data = spark.sql(f"... WHERE region = '{region}'").collect()
    bc = spark.sparkContext.broadcast(data)
    # Overhead gigante

# CERTO: Collect uma vez, agrupa no driver
all_data = spark.sql("SELECT region, id, value FROM ...").collect()
grouped = defaultdict(list)
for row in all_data:
    grouped[row['region']].append({'id': row['id'], 'value': row['value']})

bc_grouped = spark.sparkContext.broadcast(grouped)
```

---

### âŒ 3. NÃ£o Limpar Broadcast Antigos

```python
# ERRADO: Broadcasts acumulando em memÃ³ria
for batch in batches:
    bc = spark.sparkContext.broadcast(get_batch_data(batch))
    process(bc)
    # bc nunca Ã© limpo!

# CERTO: Cleanup explÃ­cito
for batch in batches:
    bc = spark.sparkContext.broadcast(get_batch_data(batch))
    try:
        process(bc)
    finally:
        bc.unpersist()  # Libera memÃ³ria dos executors
```

---

### âŒ 4. Broadcast sem ValidaÃ§Ã£o de Tamanho

```python
# ERRADO: Assumir que dados sÃ£o pequenos
data = expensive_query().collect()  # Pode ser 10GB!
bc = spark.sparkContext.broadcast(data)

# CERTO: Validar antes de broadcast
import sys

data = expensive_query().collect()
size_mb = sys.getsizeof(data) / (1024 ** 2)

if size_mb > 100:
    raise ValueError(f"âš ï¸ Dados muito grandes para broadcast: {size_mb:.2f}MB")

bc = spark.sparkContext.broadcast(data)
```

---

## Alternatives to Broadcast

### 1. AggregaÃ§Ã£o No Lugar de Collect

```python
# âŒ ERRADO
total_revenue = sum([r['revenue'] for r in df.collect()])

# âœ… CERTO
total_revenue = df.agg(sum('revenue')).first()[0]
```

---

### 2. Subquery No Lugar de Filtro Manual

```python
# âŒ ERRADO
active_ids = [r['id'] for r in spark.sql("SELECT id FROM active_users").collect()]
filtered = df.filter(col('user_id').isin(active_ids))

# âœ… CERTO: Semi-join (mais eficiente)
active_users = spark.table('active_users')
filtered = df.join(active_users, 'user_id', 'left_semi')
```

---

### 3. Window Function No Lugar de Loop

```python
# âŒ ERRADO
customers = df.select('customer_id').distinct().collect()
results = []
for c in customers:
    revenue = df.filter(col('customer_id') == c['customer_id']) \
                .agg(sum('amount')).first()[0]
    results.append((c['customer_id'], revenue))

# âœ… CERTO
from pyspark.sql.window import Window
result = df.groupBy('customer_id').agg(sum('amount').alias('total_revenue'))
```

---

## Decision Tree: Quando Usar Cada Abordagem

```
Preciso enriquecer DataFrame com dados externos?
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dados cabem em memÃ³ria (< 1GB)?      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€ Sim â†’ Ã‰ um join/lookup simples?
    â”‚           â”‚
    â”‚           â”œâ”€ Sim â†’ DataFrame + broadcast() hint âš¡ MELHOR
    â”‚           â”‚
    â”‚           â””â”€ NÃ£o â†’ Precisa lÃ³gica Python complexa?
    â”‚                      â”‚
    â”‚                      â”œâ”€ Sim â†’ collect + broadcast manual âœ… OK
    â”‚                      â””â”€ NÃ£o â†’ Refatora pra SQL nativo
    â”‚
    â””â”€ NÃ£o â†’ Dados > 1GB
               â”‚
               â””â”€ Use join normal (Spark decide shuffle vs broadcast)
```

---

## ConfiguraÃ§Ãµes Importantes

```python
# Ajustar threshold de broadcast automÃ¡tico (padrÃ£o: 10MB)
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")

# Desabilitar broadcast automÃ¡tico (forÃ§ar shuffle)
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "-1")

# Timeout para broadcast (padrÃ£o: 300s)
spark.conf.set("spark.sql.broadcastTimeout", "600")

# Comprimir dados de broadcast (padrÃ£o: jÃ¡ habilitado)
spark.conf.set("spark.broadcast.compress", "true")
```

---

## Exemplo Completo: RefatoraÃ§Ã£o

### Antes (Anti-Pattern)

```python
# CÃ³digo original do analista
regions = spark.sql("SELECT DISTINCT region FROM sales").collect()
results = []

for region_row in regions:
    region = region_row['region']
    
    # Lookup de configuraÃ§Ãµes
    config = spark.sql(f"""
        SELECT discount_rate FROM region_config 
        WHERE region = '{region}'
    """).collect()[0]
    
    # Processar vendas da regiÃ£o
    sales = spark.sql(f"""
        SELECT customer_id, amount 
        FROM sales 
        WHERE region = '{region}'
    """).collect()
    
    bc_discount = spark.sparkContext.broadcast(config['discount_rate'])
    
    @udf(returnType=DoubleType())
    def apply_discount(amount):
        return amount * (1 - bc_discount.value)
    
    df_region = spark.createDataFrame(sales)
    df_processed = df_region.withColumn('final_amount', apply_discount('amount'))
    
    results.append(df_processed)

final_df = reduce(DataFrame.union, results)
```

**Problemas**:
- âŒ Loop no driver (nÃ£o distribuÃ­do)
- âŒ MÃºltiplos collects
- âŒ UDF desnecessÃ¡ria
- âŒ Union de mÃºltiplos DataFrames (lento)

---

### Depois (Best Practice)

```python
# RefatoraÃ§Ã£o performÃ¡tica
from pyspark.sql.functions import broadcast, col, when

# 1. Join com configuraÃ§Ãµes (broadcast automÃ¡tico)
region_config = spark.table('region_config').select('region', 'discount_rate')
sales_df = spark.table('sales')

# 2. OperaÃ§Ã£o distribuÃ­da em uma passada
result = sales_df.join(
    broadcast(region_config), 
    'region', 
    'left'
).withColumn(
    'final_amount', 
    col('amount') * (1 - col('discount_rate'))
).select('customer_id', 'region', 'amount', 'final_amount')
```

**Melhorias**:
- âœ… Single-pass distribuÃ­do
- âœ… Sem collect
- âœ… Sem UDF (funÃ§Ãµes nativas)
- âœ… Catalyst otimiza join + projection
- âœ… ~100x mais rÃ¡pido

---

## Monitoramento de Broadcast

```python
# Ver broadcasts ativos
spark.sparkContext._jsc.sc().getPersistentRDDs()

# Spark UI â†’ SQL tab â†’ Detalhes do job
# Procure por: "BroadcastHashJoin" ou "BroadcastExchange"

# Logar tamanho antes de broadcast
import sys

def safe_broadcast(data, name="data"):
    size_mb = sys.getsizeof(data) / (1024 ** 2)
    print(f"ğŸ“¡ Broadcasting '{name}': {size_mb:.2f}MB")
    
    if size_mb > 500:
        raise ValueError(f"âš ï¸ {name} muito grande: {size_mb:.2f}MB")
    
    return spark.sparkContext.broadcast(data)

bc = safe_broadcast(my_data, "customer_lookup")
```

---

## Resumo: Quando Usar O QuÃª

| Caso de Uso | SoluÃ§Ã£o | Motivo |
|-------------|---------|--------|
| **Join/lookup com dados < 1GB** | `df.join(broadcast(dim), 'id')` | Catalyst otimiza, codegen rÃ¡pido |
| **Filtro por lista de IDs** | `left_semi` join ou subquery | Evita collect desnecessÃ¡rio |
| **AgregaÃ§Ãµes (sum/avg/count)** | `df.agg(...)` nativo | DistribuÃ­do, sem driver bottleneck |
| **LÃ³gica Python complexa** | `collect()` + `broadcast()` manual | Quando SQL nÃ£o expressa a lÃ³gica |
| **Ver sample dos dados** | `df.limit(10).collect()` | Seguro, quantidade controlada |
| **Dados > 1GB** | Join sem broadcast hint | Deixa Spark decidir (shuffle se necessÃ¡rio) |

---

## Key Takeaways

1. **Sempre prefira DataFrame + `broadcast()` ao invÃ©s de collect + broadcast manual**
2. Collect sÃ³ quando a lÃ³gica Ã© **impossÃ­vel em SQL/Spark nativo**
3. Valide tamanho dos dados antes de broadcast (< 100MB ideal, < 1GB mÃ¡ximo)
4. Use `unpersist()` para limpar broadcasts antigos
5. Monitore via Spark UI se broadcast estÃ¡ sendo aplicado (procure "BroadcastHashJoin")
6. Se UDF Ã© sua Ãºnica soluÃ§Ã£o, provavelmente hÃ¡ uma soluÃ§Ã£o SQL melhor

---

## ReferÃªncias

- [Spark SQL Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)
- [Broadcast Variables Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables)
- [Join Strategies](https://spark.apache.org/docs/latest/sql-performance-tuning.html#join-strategy-hints-for-sql-queries)
